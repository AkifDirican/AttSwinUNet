{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "palestinian-shadow",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import os\n",
    "import argparse\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from loader import *\n",
    "from networks.attention_swin_unet import SwinAttentionUnet as ViT_seg\n",
    "from configs import swin_attention_unet as config\n",
    "from scipy.ndimage.morphology import binary_fill_holes, binary_opening\n",
    "from sklearn.metrics import f1_score\n",
    "import pandas as pd\n",
    "import glob\n",
    "import nibabel as nib\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import copy\n",
    "import yaml\n",
    "from types import SimpleNamespace  \n",
    "from utils import load_pretrain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82a3ccc7",
   "metadata": {},
   "source": [
    "## Hyper parameters and dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "expensive-courage",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'isic_loader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      7\u001b[0m device \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      9\u001b[0m data_path \u001b[38;5;241m=\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpath_to_data\u001b[39m\u001b[38;5;124m'\u001b[39m]  \n\u001b[1;32m---> 11\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m \u001b[43misic_loader\u001b[49m(path_Data \u001b[38;5;241m=\u001b[39m data_path, train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     12\u001b[0m train_loader  \u001b[38;5;241m=\u001b[39m DataLoader(train_dataset, batch_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbatch_size_tr\u001b[39m\u001b[38;5;124m'\u001b[39m]), shuffle\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     13\u001b[0m val_dataset   \u001b[38;5;241m=\u001b[39m isic_loader(path_Data \u001b[38;5;241m=\u001b[39m data_path, train \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'isic_loader' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "config         = yaml.load(open('./config_skin.yml'), Loader=yaml.FullLoader)\n",
    "number_classes = int(config['number_classes'])\n",
    "input_channels = 3\n",
    "best_val_loss  = np.inf\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "data_path = config['path_to_data']  \n",
    "\n",
    "train_dataset = isic_loader(path_Data = data_path, train = True)\n",
    "train_loader  = DataLoader(train_dataset, batch_size = int(config['batch_size_tr']), shuffle= True)\n",
    "val_dataset   = isic_loader(path_Data = data_path, train = False)\n",
    "val_loader    = DataLoader(val_dataset, batch_size = int(config['batch_size_va']), shuffle= False)\n",
    "\n",
    "test_dataset  = isic_loader(path_Data = data_path, train = False, Test = True)\n",
    "test_loader   = DataLoader(test_dataset, batch_size = 1, shuffle= True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0767d6a3",
   "metadata": {},
   "source": [
    "# config and arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "latin-ability",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> merge config from configs/swin_tiny_patch4_window7_224_lite.yaml\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--root_path', type=str,\n",
    "                    default='./Synapse/', help='root dir for data')\n",
    "parser.add_argument('--eval_interval', type=int, default=5, help='eval interval')\n",
    "parser.add_argument('--volume_path', type=str,\n",
    "                    default='./Synapse/', help='root dir for validation volume data')\n",
    "parser.add_argument('--dataset', type=str,\n",
    "                    default='Synapse', help='experiment_name')\n",
    "parser.add_argument('--list_dir', type=str,\n",
    "                    default='./lists/lists_Synapse', help='list dir')\n",
    "parser.add_argument('--num_classes', type=int,\n",
    "                    default=9, help='output channel of network')\n",
    "parser.add_argument('--saved_model', type=str,\n",
    "                    default='./weights/weights_isic17.model' , help='output dir')                   \n",
    "parser.add_argument('--max_iterations', type=int,\n",
    "                    default=30000, help='maximum epoch number to train')\n",
    "parser.add_argument('--max_epochs', type=int,\n",
    "                    default=150, help='maximum epoch number to train')\n",
    "parser.add_argument('--batch_size', type=int,\n",
    "                    default=24, help='batch_size per gpu')\n",
    "parser.add_argument('--n_gpu', type=int, default=1, help='total gpu')\n",
    "parser.add_argument('--deterministic', type=int,  default=1,\n",
    "                    help='whether use deterministic training')\n",
    "parser.add_argument('--base_lr', type=float,  default=0.01,\n",
    "                    help='segmentation network learning rate')\n",
    "parser.add_argument('--img_size', type=int,\n",
    "                    default=224, help='input patch size of network input')\n",
    "parser.add_argument('--seed', type=int,\n",
    "                    default=1234, help='random seed')\n",
    "parser.add_argument('--cfg', type=str, default='configs/swin_tiny_patch4_window7_224_lite.yaml', metavar=\"FILE\", help='path to config file', )\n",
    "parser.add_argument(\n",
    "        \"--opts\",\n",
    "        help=\"Modify config options by adding 'KEY VALUE' pairs. \",\n",
    "        default=None,\n",
    "        nargs='+',\n",
    "    )\n",
    "parser.add_argument('--zip', action='store_true', help='use zipped dataset instead of folder dataset')\n",
    "parser.add_argument('--cache-mode', type=str, default='part', choices=['no', 'full', 'part'],\n",
    "                    help='no: no cache, '\n",
    "                            'full: cache all data, '\n",
    "                            'part: sharding the dataset into nonoverlapping pieces and only cache one piece')\n",
    "parser.add_argument('--resume', help='resume from checkpoint')\n",
    "parser.add_argument('--accumulation-steps', type=int, help=\"gradient accumulation steps\")\n",
    "parser.add_argument('--use-checkpoint', action='store_true',\n",
    "                    help=\"whether to use gradient checkpointing to save memory\")\n",
    "parser.add_argument('--amp-opt-level', type=str, default='O1', choices=['O0', 'O1', 'O2'],\n",
    "                    help='mixed precision opt level, if O0, no amp is used')\n",
    "parser.add_argument('--tag', help='tag of experiment')\n",
    "parser.add_argument('--eval', action='store_true', help='Perform evaluation only')\n",
    "parser.add_argument('--throughput', action='store_true', help='Test throughput only')\n",
    "parser.add_argument('--mode', help='Select our contribution',\n",
    "                    choices=['swin','cross_contextual_attention', default='swin')\n",
    "parser.add_argument('--skip_num', help='Select our contribution',\n",
    "                    choices=['0', '1', '2','3'], default='3'),\n",
    "parser.add_argument('--operationaddatten', help='Select our contribution',\n",
    "                    choices=['+', 'mul'], default='+')\n",
    "parser.add_argument('--attention', help='0 or 1',\n",
    "                    choices=['0',\"1\"], default=\"0\")\n",
    "\n",
    "                    \n",
    "\n",
    "args = parser.parse_args(args=[])\n",
    "if args.dataset == \"Synapse\":\n",
    "    args.root_path = os.path.join(args.root_path, \"train_npz\")\n",
    "                             \n",
    "config =  config.get_swin_unet_attention_configs().to_dict()\n",
    "config.update(vars(args))\n",
    "configs = SimpleNamespace(**config)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293ee749",
   "metadata": {},
   "source": [
    "# build model and optimizer and loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "southern-harvard",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SwinTransformerSys expand initial----depths:[2, 2, 2, 2];depths_decoder:[1, 2, 2, 2];drop_path_rate:0.2;num_classes:1\n",
      "------------------------------ 0 <class 'str'>\n",
      "mode: swin skip_num 3 cross_num 3\n",
      "pretrained_path:./pretrained_ckpt/swin_tiny_patch4_window7_224.pth\n",
      "---start load pretrained modle of swin encoder---\n"
     ]
    }
   ],
   "source": [
    "# config_model = get_config() \n",
    "Net   = ViT_seg(configs,num_classes=args.num_classes).cuda()\n",
    "Net   = load_pretrain(configs,Net)\n",
    "Net   = Net.to(device)\n",
    "if int(config['pretrained']):\n",
    "    Net.load_state_dict(torch.load(config['saved_model'], map_location='cpu')['model_weights'])\n",
    "    best_val_loss = torch.load(config['saved_model'], map_location='cpu')['val_loss']\n",
    "\n",
    "optimizer = optim.Adam(Net.parameters(), lr= float(config['lr']))\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor = 0.5, patience = config['patience'])\n",
    "criteria  = torch.nn.BCELoss()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74a1759",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dramatic-separation",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Epoch>> 1 and itteration 1 Loss>> 0.7598584294319153\n",
      " Epoch>> 1 and itteration 30 Loss>> 0.45233075122038524\n",
      " Epoch>> 1 and itteration 59 Loss>> 0.3408874323812582\n",
      " Epoch>> 2 and itteration 1 Loss>> 0.3103235065937042\n",
      " Epoch>> 2 and itteration 30 Loss>> 0.17979845528801283\n",
      " Epoch>> 2 and itteration 59 Loss>> 0.17059589619353666\n",
      " Epoch>> 3 and itteration 1 Loss>> 0.12587174773216248\n",
      " Epoch>> 3 and itteration 30 Loss>> 0.1560018355647723\n",
      " Epoch>> 3 and itteration 59 Loss>> 0.14095270254854428\n",
      " Epoch>> 4 and itteration 1 Loss>> 0.10675635188817978\n",
      " Epoch>> 4 and itteration 30 Loss>> 0.10480246866742769\n",
      " Epoch>> 4 and itteration 59 Loss>> 0.11470934066732051\n",
      " Epoch>> 5 and itteration 1 Loss>> 0.10782317817211151\n",
      " Epoch>> 5 and itteration 30 Loss>> 0.11757173538208007\n",
      " Epoch>> 5 and itteration 59 Loss>> 0.10765916816258835\n",
      "val_mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [00:04, 82.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 score (F-measure) or DSC: 0.9043919450327523\n",
      "New best loss, saving...\n",
      " Epoch>> 6 and itteration 1 Loss>> 0.09028090536594391\n",
      " Epoch>> 6 and itteration 30 Loss>> 0.08701437128086885\n",
      " Epoch>> 6 and itteration 59 Loss>> 0.09338036709922855\n",
      " Epoch>> 7 and itteration 1 Loss>> 0.09963338822126389\n",
      " Epoch>> 7 and itteration 30 Loss>> 0.09091660069922607\n",
      " Epoch>> 7 and itteration 59 Loss>> 0.08712454190698721\n",
      " Epoch>> 8 and itteration 1 Loss>> 0.059745561331510544\n",
      " Epoch>> 8 and itteration 30 Loss>> 0.08953677602112294\n",
      " Epoch>> 8 and itteration 59 Loss>> 0.08596139300172612\n",
      " Epoch>> 9 and itteration 1 Loss>> 0.07752776145935059\n",
      " Epoch>> 9 and itteration 30 Loss>> 0.08311334513127804\n",
      " Epoch>> 9 and itteration 59 Loss>> 0.08043004149350069\n",
      " Epoch>> 10 and itteration 1 Loss>> 0.07093984633684158\n",
      " Epoch>> 10 and itteration 30 Loss>> 0.07712965222696463\n",
      " Epoch>> 10 and itteration 59 Loss>> 0.07644497319045714\n",
      "val_mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [00:05, 79.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 score (F-measure) or DSC: 0.9152280165215335\n",
      "New best loss, saving...\n",
      " Epoch>> 11 and itteration 1 Loss>> 0.07130124419927597\n",
      " Epoch>> 11 and itteration 30 Loss>> 0.07121715831259887\n",
      " Epoch>> 11 and itteration 59 Loss>> 0.07280064083762088\n",
      " Epoch>> 12 and itteration 1 Loss>> 0.08941862732172012\n",
      " Epoch>> 12 and itteration 30 Loss>> 0.093370030199488\n",
      " Epoch>> 12 and itteration 59 Loss>> 0.08433971730834347\n",
      " Epoch>> 13 and itteration 1 Loss>> 0.13843557238578796\n",
      " Epoch>> 13 and itteration 30 Loss>> 0.07845850611726443\n",
      " Epoch>> 13 and itteration 59 Loss>> 0.07514385039270935\n",
      " Epoch>> 14 and itteration 1 Loss>> 0.0605207160115242\n",
      " Epoch>> 14 and itteration 30 Loss>> 0.06449461753169695\n",
      " Epoch>> 14 and itteration 59 Loss>> 0.07079538431460575\n",
      " Epoch>> 15 and itteration 1 Loss>> 0.041732873767614365\n",
      " Epoch>> 15 and itteration 30 Loss>> 0.06670719670752684\n",
      " Epoch>> 15 and itteration 59 Loss>> 0.06604485971442724\n",
      "val_mode\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "400it [00:05, 67.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "F1 score (F-measure) or DSC: 0.9172819426683395\n",
      "New best loss, saving...\n",
      " Epoch>> 16 and itteration 1 Loss>> 0.0721331313252449\n",
      " Epoch>> 16 and itteration 30 Loss>> 0.06559696520368258\n",
      " Epoch>> 16 and itteration 59 Loss>> 0.06454508890539913\n",
      " Epoch>> 17 and itteration 1 Loss>> 0.042822740972042084\n",
      " Epoch>> 17 and itteration 30 Loss>> 0.06008548500637213\n",
      " Epoch>> 17 and itteration 59 Loss>> 0.06137927040710288\n",
      " Epoch>> 18 and itteration 1 Loss>> 0.05977281555533409\n",
      " Epoch>> 18 and itteration 30 Loss>> 0.05741407809158166\n",
      " Epoch>> 18 and itteration 59 Loss>> 0.059474343591827454\n",
      " Epoch>> 19 and itteration 1 Loss>> 0.05831508710980415\n",
      " Epoch>> 19 and itteration 30 Loss>> 0.055315382033586505\n",
      " Epoch>> 19 and itteration 59 Loss>> 0.055440444814956795\n",
      " Epoch>> 20 and itteration 1 Loss>> 0.04754256457090378\n"
     ]
    }
   ],
   "source": [
    "best_F1_score = 0.0\n",
    "for ep in range(int(config['epochs'])):\n",
    "    Net.train()\n",
    "    epoch_loss = 0\n",
    "    for itter, batch in enumerate(train_loader):\n",
    "        img = batch['image'].to(device, dtype=torch.float)\n",
    "        msk = batch['mask'].to(device)\n",
    "        mask_type = torch.float32\n",
    "        msk = msk.to(device=device, dtype=mask_type)\n",
    "        msk_pred = Net(img)\n",
    "        loss          = criteria(msk_pred, msk) \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        epoch_loss += loss.item()\n",
    "        optimizer.step()  \n",
    "        if itter%int(float(config['progress_p']) * len(train_loader))==0:\n",
    "            print(f' Epoch>> {ep+1} and itteration {itter+1} Loss>> {((epoch_loss/(itter+1)))}')\n",
    "\n",
    "    predictions = []\n",
    "    gt = []\n",
    "\n",
    "    if (ep+1)%args.eval_interval==0:\n",
    "        with torch.no_grad():\n",
    "            print('val_mode')\n",
    "            val_loss = 0\n",
    "            Net.eval()\n",
    "            for itter, batch in tqdm(enumerate(test_loader)):\n",
    "                img = batch['image'].to(device, dtype=torch.float)\n",
    "                msk = batch['mask']\n",
    "                msk_pred = Net(img)\n",
    "\n",
    "                gt.append(msk.numpy()[0, 0])\n",
    "                msk_pred = msk_pred.cpu().detach().numpy()[0, 0]\n",
    "                msk_pred  = np.where(msk_pred>=0.4, 1, 0)\n",
    "                predictions.append(msk_pred)        \n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "        gt = np.array(gt)\n",
    "\n",
    "        y_scores = predictions.reshape(-1)\n",
    "        y_true   = gt.reshape(-1)\n",
    "\n",
    "        y_scores2 = np.where(y_scores>0.5, 1, 0)\n",
    "        y_true2   = np.where(y_true>0.5, 1, 0)\n",
    "\n",
    "        #F1 score\n",
    "        F1_score = f1_score(y_true2, y_scores2, labels=None, average='binary', sample_weight=None)\n",
    "        print (\"\\nF1 score (F-measure) or DSC: \" +str(F1_score))    \n",
    "        if (F1_score) > best_F1_score:\n",
    "            print('New best loss, saving...')\n",
    "            best_F1_score = copy.deepcopy(F1_score)\n",
    "            state = copy.deepcopy({'model_weights': Net.state_dict(), 'test_F1_score': F1_score})\n",
    "            torch.save(state, args.saved_model)\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "minus-google",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "personal-creativity",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
